{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Reporting\n",
    "This code produces an Excel file with weekly reporting data.\n",
    "\n",
    "### Directions:\n",
    "1. Ensure Rollforward.csv is in the proper location and updated using Galileo data\n",
    "2. Update 'Reusable Stuff.ipynb' if necessary\n",
    "3. Update FILE_PATH if necessary\n",
    "\n",
    "### To Do:\n",
    "1. Merge data with transaction table\n",
    "2. Improve way to isolate incentive checks (based off response from FIS???)\n",
    "3. Add charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import reusable variables and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'Reusable Stuff.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables for analysis\n",
    "Variables include # of weeks in a pay period, first day of first pay period, last day in dataset, accounts to exclude from analysis, and current salespeople."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory path for where to save file\n",
    "file_path = '/users/kevinkupiec/Dropbox (One Financial)/OFH Share/Reporting & Analysis/Reporting/Weekly Updates/'\n",
    "\n",
    "\n",
    "# number of weeks per period\n",
    "period_length = 1       \n",
    "\n",
    "\n",
    "# number of weeks per cohort\n",
    "cohort_length = 2   \n",
    "\n",
    "\n",
    "# rolling days in analysis\n",
    "rolling_days = 45\n",
    "\n",
    "\n",
    "# day of start of first pay period\n",
    "first_day = Timestamp('2015-06-22')\n",
    "\n",
    "\n",
    "# define last day for analysis\n",
    "last_day = rollforward2_last_day\n",
    "\n",
    "\n",
    "# create dict for PRN mapping\n",
    "with open(prn_mapping_file_name) as f:\n",
    "    f.readline() # ignore first line (header)\n",
    "    prn_mapping = dict(csv.reader(f, delimiter=','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to help complete analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create initial data tables from database\n",
    "\n",
    "Note that values in table **df_txn** currently come from Galileo data as transaction table is not yet complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to database\n",
    "db = connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input account data including signup date\n",
    "#     note: shift from TNCs = True to Galileo Enrollment = Confirmed added one signup \n",
    "#     in between June 29 & July 5, 2015; i do not believe it's a test account\n",
    "df_acct_input = get_dataframe(\n",
    "\"\"\"\n",
    "SELECT\n",
    "    a.account_owner_uid,\n",
    "    a.ach_account_number as pmt_ref_no,\n",
    "    u.created_at AS signup_date,\n",
    "    u.roles AS account_roles\n",
    "FROM accounts AS a \n",
    "INNER JOIN users AS u \n",
    "    ON u.uid = a.account_owner_uid\n",
    "LEFT OUTER JOIN partner_enrollments AS pe\n",
    "    ON a.galileo_account_enrollment_uid = pe.uid  \n",
    "WHERE a.product_uid = '00fe52dda6c028c44db8485296d6855eacc'\n",
    "    AND pe.status = 'confirmed'\n",
    "ORDER BY signup_date DESC\n",
    "\"\"\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input transaction data from Galileo - this is all posted data & date is date posted\n",
    "df_txn_input = pd.read_csv(rollforward2_file_name, parse_dates=[3])\n",
    "df_txn_input['pmt_ref_no'] = df_txn_input['pmt_ref_no'].astype(str)\n",
    "\n",
    "\n",
    "# map federal benefits PRNs\n",
    "df_txn_input['pmt_ref_no'] = df_txn_input['pmt_ref_no'].apply(lambda x: prn_mapping[x] if x in prn_mapping else x)\n",
    "\n",
    "\n",
    "# remove transactions from products we don't care about\n",
    "df_txn_input = df_txn_input[df_txn_input['prod_id'].isin((5105, 5106, 5197)) == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create working copies of tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_acct = df_acct_input.copy()\n",
    "df_txn = df_txn_input.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data & add new data fields to create working transaction and account tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create field with transaction type\n",
    "txn_type_args = (txn_type, 'activity_type', 'activity_subtype', 'amount')\n",
    "df_txn['transaction_type'] = df_txn.apply(append_derived_column, axis=1, args=txn_type_args)\n",
    "\n",
    "\n",
    "# apply check classification function to create new column; incentive check considered any check that is $1, $9.90, or $10\n",
    "check_type_args = (incentive_check, 'amount', 'activity_type', 'activity_subtype')\n",
    "df_txn['incentive_check'] = df_txn.apply(append_derived_column, axis=1, args=check_type_args)\n",
    "df_txn['transaction_type'] = df_txn.apply(lambda row: 'Incentive Check' if row['incentive_check'] == True else row['transaction_type'], axis=1)\n",
    "\n",
    "\n",
    "# add account_owner_uid & clean transaction table (will make transition to database from Galileo easier to integrate)\n",
    "df_txn = df_txn.merge(df_acct, on='pmt_ref_no', how='inner')\n",
    "df_txn = df_txn[['account_owner_uid', 'account_roles', 'row', 'posted_date', 'transaction_type', 'amount']]\n",
    "df_txn.columns = ['account_owner_uid', 'account_roles', 'transaction_uid', 'posted_at', 'transaction_type', 'amount']\n",
    "\n",
    "\n",
    "# append field that identifies loads\n",
    "load_args = (load, 'transaction_type', 'amount')\n",
    "df_txn['load_ind'] = df_txn.apply(append_derived_column, axis=1, args=load_args)\n",
    "\n",
    "\n",
    "# change check load transaction types to 'Check Deposit'\n",
    "df_txn['transaction_type'] = df_txn.apply(lambda row: 'Check Deposit' if row['transaction_type'] == 'Check Deposit (Instant)' else row['transaction_type'], axis=1)\n",
    "df_txn['transaction_type'] = df_txn.apply(lambda row: 'Check Deposit' if row['transaction_type'] == 'Check Deposit (Delayed)' else row['transaction_type'], axis=1)\n",
    "\n",
    "\n",
    "# add transaction day, transaction week, & transaction pay period to transaction table\n",
    "df_txn['txn_date'] = df_txn.posted_at.apply(lambda x: Timestamp('{}-{}-{}'.format(x.year, x.month, x.day)) if isinstance(x, Timestamp) else None)\n",
    "df_txn['txn_day'] = df_txn.apply(lambda row: days_since(row['txn_date'], first_day), axis=1)\n",
    "df_txn['txn_week'] = df_txn.apply(lambda row: weeks_since(row['txn_date'], first_day, monday_start=True), axis=1)\n",
    "df_txn['txn_month'] = df_txn.posted_at.apply(lambda x: Timestamp('{}-{}-{}'.format(x.year, x.month, x.days_in_month)) if isinstance(x, Timestamp) else None)\n",
    "\n",
    "txn_period_args = (period, 'txn_week')\n",
    "txn_period_kwargs = {'weeks' : period_length}\n",
    "df_txn['txn_period'] = df_txn.apply(append_derived_column, axis=1, args=txn_period_args, **txn_period_kwargs)\n",
    "\n",
    "\n",
    "# add signup day, signup week, & signup pay period to account table\n",
    "df_acct['signup_date'] = df_acct.signup_date.apply(lambda x: Timestamp('{}-{}-{}'.format(x.year, x.month, x.day)) if isinstance(x, Timestamp) else None)\n",
    "df_acct['signup_day'] = df_acct.apply(lambda row: days_since(row['signup_date'], first_day), axis = 1)\n",
    "df_acct['signup_week'] = df_acct.apply(lambda row: weeks_since(row['signup_date'], first_day, monday_start=True), axis = 1)\n",
    "df_acct['signup_month'] = df_acct.signup_date.apply(lambda x: Timestamp('{}-{}-{}'.format(x.year, x.month, x.days_in_month)) if isinstance(x, Timestamp) else None)\n",
    "\n",
    "signup_period_args = (period, 'signup_week')\n",
    "signup_period_kwargs = {'weeks' : period_length}\n",
    "df_acct['signup_period'] = df_acct.apply(append_derived_column, axis=1, args=signup_period_args, **signup_period_kwargs)\n",
    "\n",
    "\n",
    "# add cohort to account table\n",
    "cohort_args = (cohort, 'signup_week')\n",
    "cohort_kwargs = {'weeks' : cohort_length}\n",
    "df_acct['cohort'] = df_acct.apply(append_derived_column, axis=1, args=cohort_args, **cohort_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create clean transaction, load, and account tables by removing test accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove test accounts from transaction table\n",
    "df_txn_clean = df_txn.copy()\n",
    "df_txn_clean = df_txn_clean[df_txn_clean.account_roles.apply(lambda x: account_type(x)).isin(['Test Account']) == False]\n",
    "df_txn_clean = df_txn_clean[df_txn_clean.account_roles.apply(lambda x: account_type(x)).isin(['Employee Account']) == False]\n",
    "df_txn_clean = df_txn_clean[df_txn_clean['account_owner_uid'].isin(hq_test_users) == False]\n",
    "df_txn_clean = df_txn_clean[df_txn_clean.txn_date <= last_day]\n",
    "\n",
    "\n",
    "# remove test accounts from account table\n",
    "df_acct_clean = df_acct.copy()\n",
    "df_acct_clean = df_acct_clean[df_acct_clean.account_roles.apply(lambda x: account_type(x)).isin(['Test Account']) == False]\n",
    "df_acct_clean = df_acct_clean[df_acct_clean.account_roles.apply(lambda x: account_type(x)).isin(['Employee Account']) == False]\n",
    "df_acct_clean = df_acct_clean[df_acct_clean['account_owner_uid'].isin(hq_test_users) == False]\n",
    "df_acct_clean = df_acct_clean[df_acct_clean.signup_date <= last_day]\n",
    "\n",
    "\n",
    "# create load only table for further analysis\n",
    "df_load_clean = df_txn_clean[df_txn_clean.load_ind == True]\n",
    "\n",
    "\n",
    "# create network debit only table for further analysis\n",
    "df_net_deb_clean = df_txn_clean[df_txn_clean.transaction_type == 'Network Debit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append conversion date to account table\n",
    "Note that a load only includes all non-incentive checks, all cash loads, and only ACH transfers $5.00+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define first load period per individual\n",
    "df_first_load = df_load_clean.groupby('account_owner_uid').agg({'txn_period' : 'min', 'txn_day' : 'min', 'txn_month' : 'min'}).reset_index()\n",
    "\n",
    "\n",
    "# rename columns\n",
    "df_first_load = df_first_load.rename(columns={'txn_period' : 'conversion_period', 'txn_day' : 'conversion_day', 'txn_month' : 'conversion_month'})\n",
    "\n",
    "\n",
    "# append first load date to account table\n",
    "df_acct_clean = df_acct_clean.merge(df_first_load, on='account_owner_uid', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create summary table of cumulative signups and conversions\n",
    "Note that a load only includes all non-incentive checks, all cash loads, and only ACH transfers $5.00+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinkupiec/ipy/lib/python2.7/site-packages/ipykernel/__main__.py:12: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "# create table of incremental signups by salesperson by pay period\n",
    "df_signups_clean = df_acct_clean.groupby(['signup_period']).agg({'account_owner_uid' : 'count'}).cumsum().reset_index()\n",
    "df_signups_clean = df_signups_clean.rename(columns={'signup_period' : 'period', 'account_owner_uid' : 'signups_sum'})\n",
    "\n",
    "\n",
    "# create table of incremental conversion by salesperson by pay period\n",
    "df_conversions_clean = df_acct_clean.groupby(['conversion_period']).agg({'account_owner_uid' : 'count'}).cumsum().reset_index()\n",
    "df_conversions_clean = df_conversions_clean.rename(columns={'conversion_period' : 'period', 'account_owner_uid' : 'conversions_sum'})\n",
    "\n",
    "\n",
    "# merge tables to make calculations easier\n",
    "df_signups_conversions = df_signups_clean.merge(df_conversions_clean, on='period', how='outer').sort('period').fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add fields for first and last day of pay period\n",
    "df_signups_conversions['period_beginning'] = df_signups_conversions.apply(lambda row: first_day + datetime.timedelta(days=((7 * period_length) * row['period'])), axis=1)\n",
    "df_signups_conversions['period_ending'] = df_signups_conversions.apply(lambda row: first_day + datetime.timedelta(days=((7 * period_length) * row['period']) + (7 * period_length - 1)), axis=1)\n",
    "\n",
    "\n",
    "# remove pay periods that end past the last day in the dataset or end prior to the first day in the analysis\n",
    "df_signups_conversions = df_signups_conversions[df_signups_conversions.period_ending <= last_day].fillna(0)\n",
    "df_signups_conversions = df_signups_conversions[df_signups_conversions.period_ending >= first_day].fillna(0)\n",
    "\n",
    "\n",
    "# re-order table to make it cleaner later\n",
    "df_final = df_signups_conversions[['period', 'period_beginning', 'period_ending', 'signups_sum', 'conversions_sum']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create summary table of 45-day load\n",
    "Note that a load only includes all non-incentive checks, all cash loads, and only ACH transfers $5.00+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary table that counts loads by day, by individual\n",
    "df_load_clean_grouped = df_load_clean.groupby(['account_owner_uid', 'txn_day']).agg({'transaction_uid' : 'count'})\n",
    "df_load_clean_grouped = df_load_clean_grouped.rename(columns={'transaction_uid' : 'daily_loads'}).reset_index()\n",
    "\n",
    "\n",
    "# create new columns for daily transactions and total transactions with max being 1 as an indicator\n",
    "df_load_clean_grouped['daily_loads'] = df_load_clean_grouped.apply(lambda row: 1 if 'daily_loads' > 0 else 0, axis=1)\n",
    "\n",
    "\n",
    "# create table to be used for looping\n",
    "df_load_clean_loop = df_load_clean_grouped.copy()\n",
    "\n",
    "\n",
    "# define merge columns for loop\n",
    "merge_columns = ['account_owner_uid', 'txn_day']\n",
    "\n",
    "\n",
    "# run loop, which appends prior rolling_days - 1 days based on account owner and day to determine \n",
    "# whether load occured in rolling rolling_days day period\n",
    "for i in range(rolling_days):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    df_load_clean_loop['txn_day'] = df_load_clean_loop['txn_day'] + 1\n",
    "    df_load_clean_grouped = df_load_clean_grouped.merge(df_load_clean_loop, on=merge_columns, suffixes=('0', i), how='outer')\n",
    "    df_load_clean_grouped = df_load_clean_grouped.rename(columns={'daily_loads0': 'daily_loads'})\n",
    "\n",
    "    \n",
    "# finish calculating rolling view   \n",
    "df_load_clean_grouped = df_load_clean_grouped.fillna(0)\n",
    "df_load_clean_grouped = df_load_clean_grouped.set_index(['account_owner_uid', 'txn_day'])\n",
    "df_load_clean_grouped['rolling_loads'] = df_load_clean_grouped.max(axis=1)\n",
    "df_load_clean_grouped = df_load_clean_grouped['rolling_loads']\n",
    "df_load_clean_grouped = df_load_clean_grouped.reset_index()\n",
    "df_load_clean_rolling = df_load_clean_grouped.groupby(['txn_day']).agg({'rolling_loads' : 'sum'}).reset_index()\n",
    "\n",
    "\n",
    "# add date to table to aid joining to final table\n",
    "df_load_clean_rolling['period_ending'] = df_load_clean_rolling.apply(lambda row: first_day + datetime.timedelta(days=row['txn_day']), axis=1)\n",
    "\n",
    "\n",
    "# reorder table to aid join\n",
    "df_load_clean_rolling = df_load_clean_rolling[['period_ending', 'rolling_loads']]\n",
    "\n",
    "\n",
    "# merge with final table\n",
    "df_final = df_final.merge(df_load_clean_rolling, on='period_ending', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create summary table of 45-day network debit\n",
    "Note that this is the posted date of the transaction - excludes returns but includes adjustments that are <$0.00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create summary table that counts network debits by day, by individual\n",
    "df_net_deb_clean_grouped = df_net_deb_clean.groupby(['account_owner_uid', 'txn_day']).agg({'transaction_uid' : 'count'})\n",
    "df_net_deb_clean_grouped = df_net_deb_clean_grouped.rename(columns={'transaction_uid' : 'daily_net_debs'}).reset_index()\n",
    "\n",
    "\n",
    "# create new columns for daily transactions and total transactions with max being 1 as an indicator\n",
    "df_net_deb_clean_grouped['daily_net_debs'] = df_net_deb_clean_grouped.apply(lambda row: 1 if 'daily_net_debs' > 0 else 0, axis=1)\n",
    "\n",
    "\n",
    "# create table to be used for looping\n",
    "df_net_deb_clean_loop = df_net_deb_clean_grouped.copy()\n",
    "\n",
    "\n",
    "# define merge columns for loop\n",
    "merge_columns = ['account_owner_uid', 'txn_day']\n",
    "\n",
    "\n",
    "# run loop, which appends prior rolling_days - 1 days based on account owner and day to determine \n",
    "# whether net_deb occured in rolling rolling_days day period\n",
    "for i in range(rolling_days):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    df_net_deb_clean_loop['txn_day'] = df_net_deb_clean_loop['txn_day'] + 1\n",
    "    df_net_deb_clean_grouped = df_net_deb_clean_grouped.merge(df_net_deb_clean_loop, on=merge_columns, suffixes=('0', i), how='outer')\n",
    "    df_net_deb_clean_grouped = df_net_deb_clean_grouped.rename(columns={'daily_net_debs0': 'daily_net_debs'})\n",
    "\n",
    "    \n",
    "# finish calculating rolling view   \n",
    "df_net_deb_clean_grouped = df_net_deb_clean_grouped.fillna(0)\n",
    "df_net_deb_clean_grouped = df_net_deb_clean_grouped.set_index(['account_owner_uid', 'txn_day'])\n",
    "df_net_deb_clean_grouped['rolling_net_debs'] = df_net_deb_clean_grouped.max(axis=1)\n",
    "df_net_deb_clean_grouped = df_net_deb_clean_grouped['rolling_net_debs']\n",
    "df_net_deb_clean_grouped = df_net_deb_clean_grouped.reset_index()\n",
    "df_net_deb_clean_rolling = df_net_deb_clean_grouped.groupby(['txn_day']).agg({'rolling_net_debs' : 'sum'}).reset_index()\n",
    "\n",
    "\n",
    "# add date to table to aid joining to final table\n",
    "df_net_deb_clean_rolling['period_ending'] = df_net_deb_clean_rolling.apply(lambda row: first_day + datetime.timedelta(days=row['txn_day']), axis=1)\n",
    "\n",
    "\n",
    "# reorder table to aid join\n",
    "df_net_deb_clean_rolling = df_net_deb_clean_rolling[['period_ending', 'rolling_net_debs']]\n",
    "\n",
    "\n",
    "# merge with final table\n",
    "df_final = df_final.merge(df_net_deb_clean_rolling, on='period_ending', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create table of count of various transaction types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table of transaction sum grouping by debit type\n",
    "df_txn_clean_sum = df_txn_clean.groupby(['txn_period', 'transaction_type']).agg({'amount' : 'sum'}).unstack().fillna(0)\n",
    "df_txn_clean_sum.columns = df_txn_clean_sum.columns.droplevel(0)\n",
    "df_txn_clean_sum['Total Debit'] = df_txn_clean_sum['ATM Withdrawal'] + df_txn_clean_sum['Bill Pay'] + df_txn_clean_sum['Mail a Check'] + df_txn_clean_sum['Network Debit'] + df_txn_clean_sum['Network Credit'] + df_txn_clean_sum['ACH Debit']\n",
    "df_txn_clean_sum['Total Load'] = df_txn_clean_sum['Check Deposit'] + df_txn_clean_sum['Cash Load'] + df_txn_clean_sum['ACH Credit']\n",
    "df_txn_clean_sum['Total Transfers'] = df_txn_clean_sum['Transfer from Vault'] + df_txn_clean_sum['Transfer to Vault']\n",
    "\n",
    "\n",
    "df_sum = df_txn_clean_sum[['Network Debit', 'ATM Withdrawal', 'Total Debit', \n",
    "                                           'ACH Credit', 'Cash Load', 'Check Deposit', 'Total Load', \n",
    "                                           'Transfer to Vault', 'Transfer from Vault', 'Total Transfers']]\n",
    "df_sum = df_sum.rename(columns={'Network Debit' : 'Network Debit - Sum', \n",
    "                                                'ATM Withdrawal' : 'ATM Withdrawal - Sum', \n",
    "                                                'Total Debit' : 'Total Debit - Sum', \n",
    "                                                'ACH Credit' : 'ACH Credit - Sum', \n",
    "                                                'Cash Load' : 'Cash Load - Sum', \n",
    "                                                'Check Deposit' : 'Check Deposit - Sum', \n",
    "                                                'Total Load' : 'Total Load - Sum', \n",
    "                                                'Transfer to Vault' : 'Transfer to Vault - Sum', \n",
    "                                                'Transfer from Vault' : 'Transfer from Vault - Sum', \n",
    "                                                'Total Transfers' : 'Total Transfers - Sum'})\n",
    "df_sum.index.rename('period', inplace=True)\n",
    "df_sum = df_sum.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table of transaction count grouping by debit type\n",
    "df_txn_clean_count = df_txn_clean.groupby(['txn_period', 'transaction_type']).agg({'amount' : 'count'}).unstack().fillna(0)\n",
    "df_txn_clean_count.columns = df_txn_clean_count.columns.droplevel(0)\n",
    "df_txn_clean_count['Total Debit'] = df_txn_clean_count['ATM Withdrawal'] + df_txn_clean_count['Bill Pay'] + df_txn_clean_count['Mail a Check'] + df_txn_clean_count['Network Debit'] + df_txn_clean_count['Network Credit'] + df_txn_clean_count['ACH Debit']\n",
    "df_txn_clean_count['Total Load'] = df_txn_clean_count['Check Deposit'] + df_txn_clean_count['Cash Load'] + df_txn_clean_count['ACH Credit']\n",
    "df_txn_clean_count['Total Transfers'] = df_txn_clean_count['Transfer from Vault'] + df_txn_clean_count['Transfer to Vault']\n",
    "\n",
    "\n",
    "df_count = df_txn_clean_count[['Network Debit', 'ATM Withdrawal', 'Total Debit', \n",
    "                                           'ACH Credit', 'Cash Load', 'Check Deposit', 'Total Load', \n",
    "                                           'Transfer to Vault', 'Transfer from Vault', 'Total Transfers']]\n",
    "df_count = df_count.rename(columns={'Network Debit' : 'Network Debit - Count', \n",
    "                                                'ATM Withdrawal' : 'ATM Withdrawal - Count', \n",
    "                                                'Total Debit' : 'Total Debit - Count', \n",
    "                                                'ACH Credit' : 'ACH Credit - Count', \n",
    "                                                'Cash Load' : 'Cash Load - Count', \n",
    "                                                'Check Deposit' : 'Check Deposit - Count', \n",
    "                                                'Total Load' : 'Total Load - Count', \n",
    "                                                'Transfer to Vault' : 'Transfer to Vault - Count', \n",
    "                                                'Transfer from Vault' : 'Transfer from Vault - Count', \n",
    "                                                'Total Transfers' : 'Total Transfers - Count'})\n",
    "df_count.index.rename('period', inplace=True)\n",
    "df_count = df_count.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge onto final table\n",
    "df_final = df_final.merge(df_sum, on='period', how='left').fillna(0)\n",
    "df_final = df_final.merge(df_count, on='period', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate above for a monthly view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinkupiec/ipy/lib/python2.7/site-packages/ipykernel/__main__.py:12: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "# create table of incremental signups by salesperson by pay period\n",
    "df_signups_month_clean = df_acct_clean.groupby(['signup_month']).agg({'account_owner_uid' : 'count'}).cumsum().reset_index()\n",
    "df_signups_month_clean = df_signups_month_clean.rename(columns={'signup_month' : 'month', 'account_owner_uid' : 'signups_sum'})\n",
    "\n",
    "\n",
    "# create table of incremental conversion by salesperson by pay period\n",
    "df_conversions_month_clean = df_acct_clean.groupby(['conversion_month']).agg({'account_owner_uid' : 'count'}).cumsum().reset_index()\n",
    "df_conversions_month_clean = df_conversions_month_clean.rename(columns={'conversion_month' : 'month', 'account_owner_uid' : 'conversions_sum'})\n",
    "\n",
    "\n",
    "# merge tables to make calculations easier\n",
    "df_signups_conversions_month = df_signups_month_clean.merge(df_conversions_month_clean, on='month', how='outer').sort('month').fillna(method='ffill')\n",
    "\n",
    "\n",
    "# remove pay periods that end past the last day in the dataset or end prior to the first day in the analysis\n",
    "df_signups_conversions_month = df_signups_conversions_month[df_signups_conversions_month.month <= last_day].fillna(0)\n",
    "df_signups_conversions_month = df_signups_conversions_month[df_signups_conversions_month.month >= first_day].fillna(0)\n",
    "\n",
    "\n",
    "# re-order table to make it cleaner later\n",
    "df_final_month = df_signups_conversions_month[['month', 'signups_sum', 'conversions_sum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 45 day rolling views\n",
    "df_final_month = df_final_month.merge(df_load_clean_rolling, left_on='month', right_on='period_ending', how='left')\n",
    "df_final_month.drop('period_ending', axis=1, inplace=True)\n",
    "df_final_month = df_final_month.merge(df_net_deb_clean_rolling, left_on='month', right_on='period_ending', how='left')\n",
    "df_final_month.drop('period_ending', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create table of transaction sum grouping by debit type\n",
    "df_txn_clean_sum_month = df_txn_clean.groupby(['txn_month', 'transaction_type']).agg({'amount' : 'sum'}).unstack().fillna(0)\n",
    "df_txn_clean_sum_month.columns = df_txn_clean_sum_month.columns.droplevel(0)\n",
    "df_txn_clean_sum_month['Total Debit'] = df_txn_clean_sum_month['ATM Withdrawal'] + df_txn_clean_sum_month['Bill Pay'] + df_txn_clean_sum_month['Mail a Check'] + df_txn_clean_sum_month['Network Debit'] + df_txn_clean_sum_month['Network Credit'] + df_txn_clean_sum_month['ACH Debit']\n",
    "df_txn_clean_sum_month['Total Load'] = df_txn_clean_sum_month['Check Deposit'] + df_txn_clean_sum_month['Cash Load'] + df_txn_clean_sum_month['ACH Credit']\n",
    "df_txn_clean_sum_month['Total Transfers'] = df_txn_clean_sum_month['Transfer from Vault'] + df_txn_clean_sum_month['Transfer to Vault']\n",
    "\n",
    "\n",
    "df_sum_month = df_txn_clean_sum_month[['Network Debit', 'ATM Withdrawal', 'Total Debit', \n",
    "                                           'ACH Credit', 'Cash Load', 'Check Deposit', 'Total Load', \n",
    "                                           'Transfer to Vault', 'Transfer from Vault', 'Total Transfers']]\n",
    "df_sum_month = df_sum_month.rename(columns={'Network Debit' : 'Network Debit - Sum', \n",
    "                                                'ATM Withdrawal' : 'ATM Withdrawal - Sum', \n",
    "                                                'Total Debit' : 'Total Debit - Sum', \n",
    "                                                'ACH Credit' : 'ACH Credit - Sum', \n",
    "                                                'Cash Load' : 'Cash Load - Sum', \n",
    "                                                'Check Deposit' : 'Check Deposit - Sum', \n",
    "                                                'Total Load' : 'Total Load - Sum', \n",
    "                                                'Transfer to Vault' : 'Transfer to Vault - Sum', \n",
    "                                                'Transfer from Vault' : 'Transfer from Vault - Sum', \n",
    "                                                'Total Transfers' : 'Total Transfers - Sum'})\n",
    "df_sum_month.index.rename('month', inplace=True)\n",
    "df_sum_month = df_sum_month.reset_index()\n",
    "\n",
    "\n",
    "# create table of transaction count grouping by debit type\n",
    "df_txn_clean_count_month = df_txn_clean.groupby(['txn_month', 'transaction_type']).agg({'amount' : 'count'}).unstack().fillna(0)\n",
    "df_txn_clean_count_month.columns = df_txn_clean_count_month.columns.droplevel(0)\n",
    "df_txn_clean_count_month['Total Debit'] = df_txn_clean_count_month['ATM Withdrawal'] + df_txn_clean_count_month['Bill Pay'] + df_txn_clean_count_month['Mail a Check'] + df_txn_clean_count_month['Network Debit'] + df_txn_clean_count_month['Network Credit'] + df_txn_clean_count_month['ACH Debit']\n",
    "df_txn_clean_count_month['Total Load'] = df_txn_clean_count_month['Check Deposit'] + df_txn_clean_count_month['Cash Load'] + df_txn_clean_count_month['ACH Credit']\n",
    "df_txn_clean_count_month['Total Transfers'] = df_txn_clean_count_month['Transfer from Vault'] + df_txn_clean_count_month['Transfer to Vault']\n",
    "\n",
    "\n",
    "df_count_month = df_txn_clean_count_month[['Network Debit', 'ATM Withdrawal', 'Total Debit', \n",
    "                                           'ACH Credit', 'Cash Load', 'Check Deposit', 'Total Load', \n",
    "                                           'Transfer to Vault', 'Transfer from Vault', 'Total Transfers']]\n",
    "df_count_month = df_count_month.rename(columns={'Network Debit' : 'Network Debit - Count', \n",
    "                                                'ATM Withdrawal' : 'ATM Withdrawal - Count', \n",
    "                                                'Total Debit' : 'Total Debit - Count', \n",
    "                                                'ACH Credit' : 'ACH Credit - Count', \n",
    "                                                'Cash Load' : 'Cash Load - Count', \n",
    "                                                'Check Deposit' : 'Check Deposit - Count', \n",
    "                                                'Total Load' : 'Total Load - Count', \n",
    "                                                'Transfer to Vault' : 'Transfer to Vault - Count', \n",
    "                                                'Transfer from Vault' : 'Transfer from Vault - Count', \n",
    "                                                'Total Transfers' : 'Total Transfers - Count'})\n",
    "df_count_month.index.rename('month', inplace=True)\n",
    "df_count_month = df_count_month.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge onto final table\n",
    "df_final_month = df_final_month.merge(df_sum_month, on='month', how='left').fillna(0)\n",
    "df_final_month = df_final_month.merge(df_count_month, on='month', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create table of conversion rates by cohort\n",
    "Conversion rates are as of 2 weeks, 4 weeks, 6 weeks, & 8 weeks. Note that the first day counted is the day the individual signed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of days from signup until conversion\n",
    "df_acct_clean['days_to_convert'] = df_acct_clean['conversion_day'] - df_acct_clean['signup_day'] + 1\n",
    "\n",
    "\n",
    "# create four columns that flag whether the individual converted in 2, 4, 6, & 8 weeks and total signups in cohort\n",
    "df_acct_clean['_14'] = df_acct_clean.apply(lambda row: 1 if row['days_to_convert'] <= 14 else 0, axis=1)\n",
    "df_acct_clean['_28'] = df_acct_clean.apply(lambda row: 1 if row['days_to_convert'] <= 28 else 0, axis=1)\n",
    "df_acct_clean['_42'] = df_acct_clean.apply(lambda row: 1 if row['days_to_convert'] <= 42 else 0, axis=1)\n",
    "df_acct_clean['_56'] = df_acct_clean.apply(lambda row: 1 if row['days_to_convert'] <= 56 else 0, axis=1)\n",
    "df_acct_clean['_cumulative'] = df_acct_clean.apply(lambda row: 1 if row['days_to_convert'] >= 0 else 0, axis=1)\n",
    "df_cohort_clean = df_acct_clean.groupby('cohort').agg({'_14' : 'sum', '_28' : 'sum', '_42' : 'sum', '_56' : 'sum', '_cumulative' : 'sum', 'account_owner_uid' : 'count'}).reset_index().fillna(0)\n",
    "\n",
    "\n",
    "# add fields for first and last day of cohort\n",
    "df_cohort_clean['cohort_beginning'] = df_cohort_clean.apply(lambda row: first_day + datetime.timedelta(days=((7 * cohort_length) * row['cohort'])), axis=1)\n",
    "df_cohort_clean['cohort_ending'] = df_cohort_clean.apply(lambda row: first_day + datetime.timedelta(days=((7 * cohort_length) * row['cohort']) + (7 * cohort_length - 1)), axis=1)\n",
    "\n",
    "\n",
    "# null out fields that haven't had enough days to age\n",
    "df_cohort_clean['_14'] = df_cohort_clean.apply(lambda row: float(row['_14']) if (row['cohort_ending'] + datetime.timedelta(days=13)) <= last_day else None, axis=1)\n",
    "df_cohort_clean['_28'] = df_cohort_clean.apply(lambda row: float(row['_28']) if (row['cohort_ending'] + datetime.timedelta(days=27)) <= last_day else None, axis=1)\n",
    "df_cohort_clean['_42'] = df_cohort_clean.apply(lambda row: float(row['_42']) if (row['cohort_ending'] + datetime.timedelta(days=41)) <= last_day else None, axis=1)\n",
    "df_cohort_clean['_56'] = df_cohort_clean.apply(lambda row: float(row['_56']) if (row['cohort_ending'] + datetime.timedelta(days=55)) <= last_day else None, axis=1)\n",
    "\n",
    "\n",
    "# create conversion percentages\n",
    "df_cohort_clean['_14_pct'] = df_cohort_clean['_14'] / df_cohort_clean['account_owner_uid']\n",
    "df_cohort_clean['_28_pct'] = df_cohort_clean['_28'] / df_cohort_clean['account_owner_uid']\n",
    "df_cohort_clean['_42_pct'] = df_cohort_clean['_42'] / df_cohort_clean['account_owner_uid']\n",
    "df_cohort_clean['_56_pct'] = df_cohort_clean['_56'] / df_cohort_clean['account_owner_uid']\n",
    "df_cohort_clean['_cumulative_pct'] = df_cohort_clean['_cumulative'] / df_cohort_clean['account_owner_uid']\n",
    "\n",
    "\n",
    "# remove cohorts that end past the last day in the dataset or end prior to the first day in the analysis\n",
    "df_cohort_clean = df_cohort_clean[df_cohort_clean.cohort_ending <= last_day]\n",
    "df_cohort_clean = df_cohort_clean[df_cohort_clean.cohort_ending >= first_day]\n",
    "\n",
    "\n",
    "# create final table\n",
    "df_cohort = df_cohort_clean[['cohort', 'cohort_beginning', 'cohort_ending', 'account_owner_uid', '_14_pct', '_28_pct', '_42_pct', '_56_pct', '_cumulative_pct']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Export tables to Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinkupiec/ipy/lib/python2.7/site-packages/ipykernel/__main__.py:8: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/Users/kevinkupiec/ipy/lib/python2.7/site-packages/ipykernel/__main__.py:9: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/Users/kevinkupiec/ipy/lib/python2.7/site-packages/ipykernel/__main__.py:10: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "# update column names\n",
    "df_final = df_final.rename(columns={'period' : 'Period', 'period_beginning' : 'Period Start Date', 'period_ending' : 'Period End Date', 'signups_sum' : 'Cumulative Signups', 'conversions_sum' : 'Conversions Sum', 'rolling_loads' : 'Rolling Loads', 'rolling_net_debs' : 'Rolling Network Debits'})\n",
    "df_final_month = df_final_month.rename(columns={'month' : 'Month', 'signups_sum' : 'Cumulative Signups', 'conversions_sum' : 'Conversions Sum', 'rolling_loads' : 'Rolling Loads', 'rolling_net_debs' : 'Rolling Network Debits'})\n",
    "df_cohort = df_cohort.rename(columns={'cohort' : 'Cohort', 'cohort_beginning' : 'Cohort Start Date', 'cohort_ending' : 'Cohort End Date', 'account_owner_uid' : 'Signups', '_14_pct' : 'Two Week Conversion Rate', '_28_pct' : 'Four Week Conversion Rate', '_42_pct' : 'Six Week Conversion Rate', '_56_pct' : 'Eight Week Conversion Rate', '_cumulative_pct' : 'Cumulative Conversion Rate'})\n",
    "\n",
    "\n",
    "# sort tables\n",
    "df_final = df_final.sort('Period')\n",
    "df_final_month = df_final_month.sort('Month')\n",
    "df_cohort = df_cohort.sort('Cohort')\n",
    "\n",
    "\n",
    "# define file name\n",
    "file_name = file_path + last_day.strftime('%Y%m%d') + ' Weekly Reporting.xlsx'\n",
    "\n",
    "\n",
    "# write to file\n",
    "writer = pd.ExcelWriter(file_name)\n",
    "df_final.to_excel(writer, sheet_name='Data by Period')\n",
    "df_final_month.to_excel(writer, sheet_name='Data by Month')\n",
    "df_cohort.to_excel(writer, sheet_name='Cohort Conversion Rates')\n",
    "df_ach_cohort.to_excel(writer, sheet_name='Cohort ACH Conversion Rates')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to identify loads that count towards employee\n",
    "def ach_load(transaction_type, amount):\n",
    "    if transaction_type == 'ACH Credit' and amount >= 5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "# append field that identifies loads\n",
    "ach_load_args = (ach_load, 'transaction_type', 'amount')\n",
    "df_txn['ach_load_ind'] = df_txn.apply(append_derived_column, axis=1, args=ach_load_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_acct_clean = df_acct.copy()\n",
    "df_acct_clean = df_acct_clean[df_acct_clean.account_roles.apply(lambda x: account_type(x)).isin(['Test Account']) == False]\n",
    "df_acct_clean = df_acct_clean[df_acct_clean.account_roles.apply(lambda x: account_type(x)).isin(['Employee Account']) == False]\n",
    "df_acct_clean = df_acct_clean[df_acct_clean['account_owner_uid'].isin(hq_test_users) == False]\n",
    "df_acct_clean = df_acct_clean[df_acct_clean.signup_date <= last_day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create load only table for further analysis\n",
    "df_ach_load_clean = df_txn[df_txn.ach_load_ind == True]\n",
    "\n",
    "\n",
    "# define first load period per individual\n",
    "df_first_ach_load = df_ach_load_clean.groupby('account_owner_uid').agg({'txn_period' : 'min', 'txn_day' : 'min', 'txn_month' : 'min'}).reset_index()\n",
    "\n",
    "\n",
    "# rename columns\n",
    "df_first_ach_load = df_first_ach_load.rename(columns={'txn_period' : 'ach_conversion_period', 'txn_day' : 'ach_conversion_day', 'txn_month' : 'ach_conversion_month'})\n",
    "\n",
    "\n",
    "# append first load date to account table\n",
    "df_acct_clean = df_acct_clean.merge(df_first_ach_load, on='account_owner_uid', how='left')\n",
    "\n",
    "\n",
    "# calculate number of days from signup until conversion\n",
    "df_acct_clean['ach_days_to_convert'] = df_acct_clean['ach_conversion_day'] - df_acct_clean['signup_day'] + 1\n",
    "\n",
    "\n",
    "# create four columns that flag whether the individual converted in 2, 4, 6, & 8 weeks and total signups in cohort\n",
    "df_acct_clean['ach_14'] = df_acct_clean.apply(lambda row: 1 if row['ach_days_to_convert'] <= 14 else 0, axis=1)\n",
    "df_acct_clean['ach_28'] = df_acct_clean.apply(lambda row: 1 if row['ach_days_to_convert'] <= 28 else 0, axis=1)\n",
    "df_acct_clean['ach_42'] = df_acct_clean.apply(lambda row: 1 if row['ach_days_to_convert'] <= 42 else 0, axis=1)\n",
    "df_acct_clean['ach_56'] = df_acct_clean.apply(lambda row: 1 if row['ach_days_to_convert'] <= 56 else 0, axis=1)\n",
    "df_acct_clean['ach_cumulative'] = df_acct_clean.apply(lambda row: 1 if row['ach_days_to_convert'] >= 0 else 0, axis=1)\n",
    "df_ach_cohort_clean = df_acct_clean.groupby('cohort').agg({'ach_14' : 'sum', 'ach_28' : 'sum', 'ach_42' : 'sum', 'ach_56' : 'sum', 'ach_cumulative' : 'sum', 'account_owner_uid' : 'count'}).reset_index().fillna(0)\n",
    "\n",
    "\n",
    "# add fields for first and last day of cohort\n",
    "df_ach_cohort_clean['cohort_beginning'] = df_ach_cohort_clean.apply(lambda row: first_day + datetime.timedelta(days=((7 * cohort_length) * row['cohort'])), axis=1)\n",
    "df_ach_cohort_clean['cohort_ending'] = df_ach_cohort_clean.apply(lambda row: first_day + datetime.timedelta(days=((7 * cohort_length) * row['cohort']) + (7 * cohort_length - 1)), axis=1)\n",
    "\n",
    "\n",
    "# null out fields that haven't had enough days to age\n",
    "df_ach_cohort_clean['ach_14'] = df_ach_cohort_clean.apply(lambda row: float(row['ach_14']) if (row['cohort_ending'] + datetime.timedelta(days=13)) <= last_day else None, axis=1)\n",
    "df_ach_cohort_clean['ach_28'] = df_ach_cohort_clean.apply(lambda row: float(row['ach_28']) if (row['cohort_ending'] + datetime.timedelta(days=27)) <= last_day else None, axis=1)\n",
    "df_ach_cohort_clean['ach_42'] = df_ach_cohort_clean.apply(lambda row: float(row['ach_42']) if (row['cohort_ending'] + datetime.timedelta(days=41)) <= last_day else None, axis=1)\n",
    "df_ach_cohort_clean['ach_56'] = df_ach_cohort_clean.apply(lambda row: float(row['ach_56']) if (row['cohort_ending'] + datetime.timedelta(days=55)) <= last_day else None, axis=1)\n",
    "\n",
    "\n",
    "# create conversion percentages\n",
    "df_ach_cohort_clean['_14_pct'] = df_ach_cohort_clean['ach_14'] / df_ach_cohort_clean['account_owner_uid']\n",
    "df_ach_cohort_clean['_28_pct'] = df_ach_cohort_clean['ach_28'] / df_ach_cohort_clean['account_owner_uid']\n",
    "df_ach_cohort_clean['_42_pct'] = df_ach_cohort_clean['ach_42'] / df_ach_cohort_clean['account_owner_uid']\n",
    "df_ach_cohort_clean['_56_pct'] = df_ach_cohort_clean['ach_56'] / df_ach_cohort_clean['account_owner_uid']\n",
    "df_ach_cohort_clean['_cumulative_pct'] = df_ach_cohort_clean['ach_cumulative'] / df_ach_cohort_clean['account_owner_uid']\n",
    "\n",
    "\n",
    "# remove cohorts that end past the last day in the dataset or end prior to the first day in the analysis\n",
    "df_ach_cohort_clean = df_ach_cohort_clean[df_ach_cohort_clean.cohort_ending <= last_day]\n",
    "df_ach_cohort_clean = df_ach_cohort_clean[df_ach_cohort_clean.cohort_ending >= first_day]\n",
    "\n",
    "\n",
    "# create final table\n",
    "df_ach_cohort = df_ach_cohort_clean[['cohort', 'cohort_beginning', 'cohort_ending', 'account_owner_uid', '_14_pct', '_28_pct', '_42_pct', '_56_pct', '_cumulative_pct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
